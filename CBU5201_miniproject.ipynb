{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# A truth-false discriminator for speech stories based on multimodal neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaGn4ICrfqXZ"
      },
      "source": [
        "# 1 Author\n",
        "\n",
        "**Student Name**:  Boshi Li\n",
        "\n",
        "**Student ID**:  221171442\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o38VQkcdKd6k"
      },
      "source": [
        "# 2 Problem formulation\n",
        "\n",
        "Describe the machine learning problem that you want to solve and explain what's interesting about it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPTSuaB9L2jU"
      },
      "source": [
        "# 3 Methodology\n",
        "\n",
        "Describe your methodology. Specifically, describe your training task and validation task, and how model performance is defined (i.e. accuracy, confusion matrix, etc). Any other tasks that might help you build your model should also be described here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3BwrtEdLDit"
      },
      "source": [
        "# 4 Implemented ML prediction pipelines\n",
        "\n",
        "Describe the ML prediction pipelines that you will explore. Clearly identify their input and output, stages and format of the intermediate data structures moving from one stage to the next. It's up to you to decide which stages to include in your pipeline. After providing an overview, describe in more detail each one of the stages that you have included in their corresponding subsections (i.e. 4.1 Transformation stage, 4.2 Model stage, 4.3 Ensemble stage)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1nDXnzYLLH6"
      },
      "source": [
        "## 4.1 Transformation stage\n",
        "\n",
        "Describe any transformations, such as feature extraction. Identify input and output. Explain why you have chosen this transformation stage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n",
            "[[-0.15500641  0.09710078 -0.02004043 ...  0.01646413 -0.21695724\n",
            "  -0.09966885]\n",
            " [ 0.33452433  0.0429423   0.04421687 ... -0.3241775  -0.12838718\n",
            "  -0.23485267]]\n",
            "size of mfcc: 13\n",
            "[-6.0513245e+02  1.1406797e+02  3.4237747e+01  2.6508791e+01\n",
            "  1.4176401e+01 -8.3241564e-01 -2.4442281e-01  1.9840821e+00\n",
            "  3.5147817e+00  1.4265852e+00  8.3669430e-01  5.7564993e+00\n",
            "  2.3727574e+00]\n",
            "size of f0: 1\n",
            "124.44216273197868\n",
            "size of formant: 5\n",
            "[801.4843679519009, 1901.0915959809238, 3143.202140521808, 4180.228769356139, 4755.338048883702]\n",
            "size of intensity: 1\n",
            "39.237148015087534\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\86156\\AppData\\Local\\Temp\\ipykernel_11524\\3035497055.py:147: FutureWarning: Series.bool is now deprecated and will be removed in future version of pandas\n",
            "  if (df.loc[df['filename']==file_name]['Language']=='English').bool():\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "import numpy as np\n",
        "import parselmouth\n",
        "import librosa\n",
        "from transformers import BertModel, BertTokenizer\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def extract_f0(audio_file):\n",
        "    sound = parselmouth.Sound(audio_file)\n",
        "    pitch = sound.to_pitch()\n",
        "    f0_values = pitch.selected_array['frequency']\n",
        "    f0_values = f0_values[f0_values != 0]  # 去除无声部分\n",
        "    return np.mean(f0_values)\n",
        "\n",
        "def extract_formants(audio_file):\n",
        "    sound = parselmouth.Sound(audio_file)\n",
        "    formant = sound.to_formant_burg()\n",
        "    formant_values = []\n",
        "    for i in range(1, 6):\n",
        "        values = [formant.get_value_at_time(i, t) for t in formant.ts()]\n",
        "        values = [v for v in values if not np.isnan(v)]  # 过滤掉 NaN 值\n",
        "        if values:\n",
        "            formant_values.append(np.mean(values))\n",
        "        else:\n",
        "            formant_values.append(0)  # 如果没有有效值，用 0 替换\n",
        "    return formant_values\n",
        "\n",
        "def extract_intensity(audio_file):\n",
        "    sound = parselmouth.Sound(audio_file)\n",
        "    intensity = sound.to_intensity()\n",
        "    return np.mean(intensity.values)\n",
        "\n",
        "def extract_mfcc(audio_file, n_mfcc=13):\n",
        "    y, sr = librosa.load(audio_file, sr=None)\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
        "    return mfcc.mean(axis=1)\n",
        "\n",
        "def extract_audio_features(audio_file):\n",
        "    mfcc = extract_mfcc(audio_file)\n",
        "    f0 = extract_f0(audio_file)\n",
        "    formant = extract_formants(audio_file)\n",
        "    intensity = extract_intensity(audio_file)\n",
        "    \n",
        "    return {\n",
        "        'mfcc': mfcc,\n",
        "        'f0': f0,\n",
        "        'formant': formant,\n",
        "        'intensity': intensity\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 使用预训练的 BERT 模型提取文本特征\n",
        "class TextFeatureExtractor:\n",
        "    def __init__(self):\n",
        "        # 指定本地模型目录\n",
        "        model_dir = r'D:\\books\\machine_learning\\project\\bert-base-uncased'\n",
        "        \n",
        "        # 从本地加载模型和分词器\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
        "        self.model = BertModel.from_pretrained(model_dir)\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model.to(self.device)  # 将模型移动到 GPU\n",
        "        self.model.eval()  # 设置为评估模式，不训练\n",
        "        print(self.device)\n",
        "\n",
        "    def extract_text_features(self, text):\n",
        "        inputs = self.tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "        inputs = {key: value.to(self.device) for key, value in inputs.items()}  # 将输入移动到 GPU\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "        return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()  # 将结果移动回 CPU\n",
        "\n",
        "text_extractor = TextFeatureExtractor()\n",
        "\n",
        "texts = ['hello', 'world']\n",
        "\n",
        "text_features = np.array([text_extractor.extract_text_features(t) for t in texts])\n",
        "\n",
        "print(text_features)\n",
        "\n",
        "\n",
        "data_file=r'D:\\books\\machine_learning\\project\\data\\CBU0521DD_stories'\n",
        "\n",
        "df=pd.read_csv(r'D:\\books\\machine_learning\\project\\data\\CBU0521DD_stories_attributes.csv')\n",
        "\n",
        "\n",
        "audio_latent = []  # 语音文件列表\n",
        "texts_latent = []  # 对应的文本列表\n",
        "labels = []  # 对应的标签，0表示假，1表示真\n",
        "\n",
        "for i in range(1, 101):\n",
        "    # 生成文件名\n",
        "    file_number = str(i).zfill(5)\n",
        "    file_name = file_number + \".wav\"\n",
        "    audio_file_path = os.path.join(data_file, f\"{file_number}.wav\")\n",
        "    text_file_path = os.path.join(data_file, f\"{file_number}.txt\")\n",
        "\n",
        "    audio_latents=extract_audio_features(audio_file_path)\n",
        "    mfcc = audio_latents['mfcc']\n",
        "    f0 = audio_latents['f0']\n",
        "    formant = audio_latents['formant']\n",
        "    intensity = audio_latents['intensity']\n",
        "    \n",
        "    if len(mfcc)!= (13,):\n",
        "        mfcc = np.resize(mfcc, (13,))\n",
        "    if f0.shape != (1,):\n",
        "        f0 = np.resize(f0, (1,))\n",
        "    if len(formant) != (5,):\n",
        "        formant = np.resize(formant, (5,))\n",
        "    if intensity.shape != (1,):\n",
        "        intensity = np.resize(intensity, (1,))\n",
        "\n",
        "\n",
        "    if i==1:\n",
        "        print('size of mfcc:',len(audio_latents['mfcc']))\n",
        "        print(audio_latents['mfcc'])\n",
        "        print('size of f0:',1)\n",
        "        print(audio_latents['f0'])\n",
        "        print('size of formant:',len(audio_latents['formant']))\n",
        "        print(audio_latents['formant'])\n",
        "        print('size of intensity:',1)\n",
        "        print(audio_latents['intensity'])\n",
        "\n",
        "\n",
        "    concatenated_features = np.concatenate((mfcc, f0, formant, intensity))\n",
        "    audio_latent.append(concatenated_features)\n",
        "\n",
        "    texts_latent.append(text_extractor.extract_text_features(text_file_path))\n",
        "\n",
        "    if (df.loc[df['filename']==file_name]['Language']=='English').bool():\n",
        "        labels.append(0)\n",
        "    else:\n",
        "        labels.append(1)\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "# normalize\n",
        "audio_latent = (audio_latent - np.mean(audio_latent, axis=0)) / np.std(audio_latent, axis=0)\n",
        "texts_latent = (texts_latent - np.mean(texts_latent, axis=0)) / np.std(texts_latent, axis=0)\n",
        "\n",
        "\n",
        "audio_features=torch.tensor(audio_latent, dtype=torch.float32)\n",
        "text_features=torch.tensor(texts_latent, dtype=torch.float32)\n",
        "labels=torch.tensor(labels, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([100, 20])\n",
            "torch.Size([100, 768])\n"
          ]
        }
      ],
      "source": [
        "print(audio_features.shape)\n",
        "print(text_features.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.2 Model stage\n",
        "\n",
        "Describe the ML model(s) that you will build. Explain why you have chosen them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Training Loss: 2.1478028297424316\n",
            "Epoch 1, Test Accuracy: 45.00%\n",
            "Epoch 2, Training Loss: 1.6925914287567139\n",
            "Epoch 2, Test Accuracy: 60.00%\n",
            "Epoch 3, Training Loss: 1.3941954374313354\n",
            "Epoch 3, Test Accuracy: 70.00%\n",
            "Epoch 4, Training Loss: 1.2448742389678955\n",
            "Epoch 4, Test Accuracy: 75.00%\n",
            "Epoch 5, Training Loss: 0.9970323443412781\n",
            "Epoch 5, Test Accuracy: 70.00%\n",
            "Epoch 6, Training Loss: 0.7129600048065186\n",
            "Epoch 6, Test Accuracy: 65.00%\n",
            "Epoch 7, Training Loss: 0.6703973412513733\n",
            "Epoch 7, Test Accuracy: 75.00%\n",
            "Epoch 8, Training Loss: 0.5839830636978149\n",
            "Epoch 8, Test Accuracy: 70.00%\n",
            "Epoch 9, Training Loss: 0.6772456765174866\n",
            "Epoch 9, Test Accuracy: 70.00%\n",
            "Epoch 10, Training Loss: 0.4170570373535156\n",
            "Epoch 10, Test Accuracy: 70.00%\n",
            "Epoch 11, Training Loss: 0.4402984380722046\n",
            "Epoch 11, Test Accuracy: 80.00%\n",
            "Epoch 12, Training Loss: 0.47722959518432617\n",
            "Epoch 12, Test Accuracy: 70.00%\n",
            "Epoch 13, Training Loss: 0.36313578486442566\n",
            "Epoch 13, Test Accuracy: 65.00%\n",
            "Epoch 14, Training Loss: 0.3830522894859314\n",
            "Epoch 14, Test Accuracy: 70.00%\n",
            "Epoch 15, Training Loss: 0.34990549087524414\n",
            "Epoch 15, Test Accuracy: 70.00%\n",
            "Epoch 16, Training Loss: 0.2859272062778473\n",
            "Epoch 16, Test Accuracy: 70.00%\n",
            "Epoch 17, Training Loss: 0.282372385263443\n",
            "Epoch 17, Test Accuracy: 70.00%\n",
            "Epoch 18, Training Loss: 0.255931556224823\n",
            "Epoch 18, Test Accuracy: 65.00%\n",
            "Epoch 19, Training Loss: 0.23598818480968475\n",
            "Epoch 19, Test Accuracy: 65.00%\n",
            "Epoch 20, Training Loss: 0.22478848695755005\n",
            "Epoch 20, Test Accuracy: 65.00%\n",
            "Epoch 21, Training Loss: 0.22102007269859314\n",
            "Epoch 21, Test Accuracy: 70.00%\n",
            "Epoch 22, Training Loss: 0.20591816306114197\n",
            "Epoch 22, Test Accuracy: 70.00%\n",
            "Epoch 23, Training Loss: 0.1970297247171402\n",
            "Epoch 23, Test Accuracy: 70.00%\n",
            "Epoch 24, Training Loss: 0.20233787596225739\n",
            "Epoch 24, Test Accuracy: 65.00%\n",
            "Epoch 25, Training Loss: 0.18679019808769226\n",
            "Epoch 25, Test Accuracy: 70.00%\n",
            "Epoch 26, Training Loss: 0.19054625928401947\n",
            "Epoch 26, Test Accuracy: 75.00%\n",
            "Epoch 27, Training Loss: 0.1713697910308838\n",
            "Epoch 27, Test Accuracy: 65.00%\n",
            "Epoch 28, Training Loss: 0.16905640065670013\n",
            "Epoch 28, Test Accuracy: 70.00%\n",
            "Epoch 29, Training Loss: 0.1640743911266327\n",
            "Epoch 29, Test Accuracy: 60.00%\n",
            "Epoch 30, Training Loss: 0.1702136993408203\n",
            "Epoch 30, Test Accuracy: 65.00%\n",
            "Epoch 31, Training Loss: 0.21379806101322174\n",
            "Epoch 31, Test Accuracy: 70.00%\n",
            "Epoch 32, Training Loss: 0.5315965414047241\n",
            "Epoch 32, Test Accuracy: 65.00%\n",
            "Epoch 33, Training Loss: 0.15584662556648254\n",
            "Epoch 33, Test Accuracy: 65.00%\n",
            "Epoch 34, Training Loss: 0.74838787317276\n",
            "Epoch 34, Test Accuracy: 70.00%\n",
            "Epoch 35, Training Loss: 0.3813292682170868\n",
            "Epoch 35, Test Accuracy: 70.00%\n",
            "Epoch 36, Training Loss: 0.2821182310581207\n",
            "Epoch 36, Test Accuracy: 75.00%\n",
            "Epoch 37, Training Loss: 0.3438563942909241\n",
            "Epoch 37, Test Accuracy: 65.00%\n",
            "Epoch 38, Training Loss: 0.22368429601192474\n",
            "Epoch 38, Test Accuracy: 70.00%\n",
            "Epoch 39, Training Loss: 0.21092115342617035\n",
            "Epoch 39, Test Accuracy: 65.00%\n",
            "Epoch 40, Training Loss: 0.22190813720226288\n",
            "Epoch 40, Test Accuracy: 70.00%\n",
            "Epoch 41, Training Loss: 0.19342993199825287\n",
            "Epoch 41, Test Accuracy: 75.00%\n",
            "Epoch 42, Training Loss: 0.1888265162706375\n",
            "Epoch 42, Test Accuracy: 65.00%\n",
            "Epoch 43, Training Loss: 0.18239174783229828\n",
            "Epoch 43, Test Accuracy: 65.00%\n",
            "Epoch 44, Training Loss: 0.16908663511276245\n",
            "Epoch 44, Test Accuracy: 60.00%\n",
            "Epoch 45, Training Loss: 0.16514885425567627\n",
            "Epoch 45, Test Accuracy: 65.00%\n",
            "Epoch 46, Training Loss: 0.16179293394088745\n",
            "Epoch 46, Test Accuracy: 70.00%\n",
            "Epoch 47, Training Loss: 0.15546514093875885\n",
            "Epoch 47, Test Accuracy: 65.00%\n",
            "Epoch 48, Training Loss: 0.1499505490064621\n",
            "Epoch 48, Test Accuracy: 60.00%\n",
            "Epoch 49, Training Loss: 0.14736077189445496\n",
            "Epoch 49, Test Accuracy: 65.00%\n",
            "Epoch 50, Training Loss: 0.13392578065395355\n",
            "Epoch 50, Test Accuracy: 65.00%\n",
            "Epoch 51, Training Loss: 0.13168081641197205\n",
            "Epoch 51, Test Accuracy: 65.00%\n",
            "Epoch 52, Training Loss: 0.13839244842529297\n",
            "Epoch 52, Test Accuracy: 60.00%\n",
            "Epoch 53, Training Loss: 0.13281172513961792\n",
            "Epoch 53, Test Accuracy: 55.00%\n",
            "Epoch 54, Training Loss: 0.1264437437057495\n",
            "Epoch 54, Test Accuracy: 70.00%\n",
            "Epoch 55, Training Loss: 0.13467755913734436\n",
            "Epoch 55, Test Accuracy: 60.00%\n",
            "Epoch 56, Training Loss: 0.12229695916175842\n",
            "Epoch 56, Test Accuracy: 60.00%\n",
            "Epoch 57, Training Loss: 0.1244427040219307\n",
            "Epoch 57, Test Accuracy: 60.00%\n",
            "Epoch 58, Training Loss: 0.12206637859344482\n",
            "Epoch 58, Test Accuracy: 60.00%\n",
            "Epoch 59, Training Loss: 0.12129347771406174\n",
            "Epoch 59, Test Accuracy: 55.00%\n",
            "Epoch 60, Training Loss: 0.1165192723274231\n",
            "Epoch 60, Test Accuracy: 60.00%\n",
            "Epoch 61, Training Loss: 0.12205967307090759\n",
            "Epoch 61, Test Accuracy: 70.00%\n",
            "Epoch 62, Training Loss: 0.11692608147859573\n",
            "Epoch 62, Test Accuracy: 65.00%\n",
            "Epoch 63, Training Loss: 0.1238701120018959\n",
            "Epoch 63, Test Accuracy: 55.00%\n",
            "Epoch 64, Training Loss: 0.1679249107837677\n",
            "Epoch 64, Test Accuracy: 60.00%\n",
            "Epoch 65, Training Loss: 0.225917249917984\n",
            "Epoch 65, Test Accuracy: 65.00%\n",
            "Epoch 66, Training Loss: 0.5074172019958496\n",
            "Epoch 66, Test Accuracy: 60.00%\n",
            "Epoch 67, Training Loss: 0.8413306474685669\n",
            "Epoch 67, Test Accuracy: 60.00%\n",
            "Epoch 68, Training Loss: 0.30054178833961487\n",
            "Epoch 68, Test Accuracy: 70.00%\n",
            "Epoch 69, Training Loss: 0.23375259339809418\n",
            "Epoch 69, Test Accuracy: 55.00%\n",
            "Epoch 70, Training Loss: 0.24282307922840118\n",
            "Epoch 70, Test Accuracy: 70.00%\n",
            "Epoch 71, Training Loss: 0.22583763301372528\n",
            "Epoch 71, Test Accuracy: 55.00%\n",
            "Epoch 72, Training Loss: 0.21459577977657318\n",
            "Epoch 72, Test Accuracy: 60.00%\n",
            "Epoch 73, Training Loss: 0.21378225088119507\n",
            "Epoch 73, Test Accuracy: 60.00%\n",
            "Epoch 74, Training Loss: 0.21901993453502655\n",
            "Epoch 74, Test Accuracy: 55.00%\n",
            "Epoch 75, Training Loss: 0.1797792911529541\n",
            "Epoch 75, Test Accuracy: 55.00%\n",
            "Epoch 76, Training Loss: 0.17521148920059204\n",
            "Epoch 76, Test Accuracy: 55.00%\n",
            "Epoch 77, Training Loss: 0.1757659763097763\n",
            "Epoch 77, Test Accuracy: 55.00%\n",
            "Epoch 78, Training Loss: 0.1559111475944519\n",
            "Epoch 78, Test Accuracy: 55.00%\n",
            "Epoch 79, Training Loss: 0.1556166708469391\n",
            "Epoch 79, Test Accuracy: 55.00%\n",
            "Epoch 80, Training Loss: 0.1550149917602539\n",
            "Epoch 80, Test Accuracy: 55.00%\n",
            "Epoch 81, Training Loss: 0.14958533644676208\n",
            "Epoch 81, Test Accuracy: 55.00%\n",
            "Epoch 82, Training Loss: 0.14361850917339325\n",
            "Epoch 82, Test Accuracy: 55.00%\n",
            "Epoch 83, Training Loss: 0.1401129513978958\n",
            "Epoch 83, Test Accuracy: 55.00%\n",
            "Epoch 84, Training Loss: 0.13998857140541077\n",
            "Epoch 84, Test Accuracy: 55.00%\n",
            "Epoch 85, Training Loss: 0.13263238966464996\n",
            "Epoch 85, Test Accuracy: 55.00%\n",
            "Epoch 86, Training Loss: 0.13427968323230743\n",
            "Epoch 86, Test Accuracy: 55.00%\n",
            "Epoch 87, Training Loss: 0.12946897745132446\n",
            "Epoch 87, Test Accuracy: 60.00%\n",
            "Epoch 88, Training Loss: 0.12927284836769104\n",
            "Epoch 88, Test Accuracy: 55.00%\n",
            "Epoch 89, Training Loss: 0.12428974360227585\n",
            "Epoch 89, Test Accuracy: 60.00%\n",
            "Epoch 90, Training Loss: 0.12705270946025848\n",
            "Epoch 90, Test Accuracy: 60.00%\n",
            "Epoch 91, Training Loss: 0.1262497454881668\n",
            "Epoch 91, Test Accuracy: 60.00%\n",
            "Epoch 92, Training Loss: 0.12060563266277313\n",
            "Epoch 92, Test Accuracy: 60.00%\n",
            "Epoch 93, Training Loss: 0.12241745740175247\n",
            "Epoch 93, Test Accuracy: 60.00%\n",
            "Epoch 94, Training Loss: 0.11837002635002136\n",
            "Epoch 94, Test Accuracy: 60.00%\n",
            "Epoch 95, Training Loss: 0.11813662201166153\n",
            "Epoch 95, Test Accuracy: 60.00%\n",
            "Epoch 96, Training Loss: 0.11839106678962708\n",
            "Epoch 96, Test Accuracy: 60.00%\n",
            "Epoch 97, Training Loss: 0.11552907526493073\n",
            "Epoch 97, Test Accuracy: 60.00%\n",
            "Epoch 98, Training Loss: 0.11682066321372986\n",
            "Epoch 98, Test Accuracy: 60.00%\n",
            "Epoch 99, Training Loss: 0.11684287339448929\n",
            "Epoch 99, Test Accuracy: 60.00%\n",
            "Epoch 100, Training Loss: 0.11407565325498581\n",
            "Epoch 100, Test Accuracy: 60.00%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 定义特征映射的 MLP 模型\n",
        "class FeatureMapper(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(FeatureMapper, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, output_dim)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# 定义二分类的 MLP 模型\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 2)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# 初始化模型\n",
        "audio_mapper = FeatureMapper(audio_features.shape[1], 64)\n",
        "text_mapper = FeatureMapper(text_features.shape[1], 64)\n",
        "classifier = Classifier(128)\n",
        "\n",
        "# 定义损失函数和优化器\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(list(audio_mapper.parameters()) + list(text_mapper.parameters()) + list(classifier.parameters()), lr=0.001)\n",
        "\n",
        "# 创建数据集和数据加载器\n",
        "dataset = TensorDataset(audio_features, text_features, labels)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "# 训练模型\n",
        "for epoch in range(20):\n",
        "    audio_mapper.train()\n",
        "    text_mapper.train()\n",
        "    classifier.train()\n",
        "    \n",
        "    for audio, text, label in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        audio_mapped = audio_mapper(audio)\n",
        "        text_mapped = text_mapper(text)\n",
        "        combined_features = torch.cat((audio_mapped, text_mapped), dim=1)\n",
        "        \n",
        "        output = classifier(combined_features)\n",
        "        loss = criterion(output, label)\n",
        "\n",
        "         # 加入L2正则化项\n",
        "        l2_lambda = 0.01\n",
        "        l2_norm = sum(p.pow(2.0).sum() for p in list(audio_mapper.parameters()) + list(text_mapper.parameters()) + list(classifier.parameters()))\n",
        "        loss = loss + l2_lambda * l2_norm\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    print(f'Epoch {epoch+1}, Training Loss: {loss.item()}')\n",
        "\n",
        "    # 在测试集上评估模型\n",
        "    audio_mapper.eval()\n",
        "    text_mapper.eval()\n",
        "    classifier.eval()\n",
        "    \n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for audio, text, label in test_loader:\n",
        "            audio_mapped = audio_mapper(audio)\n",
        "            text_mapped = text_mapper(text)\n",
        "            combined_features = torch.cat((audio_mapped, text_mapped), dim=1)\n",
        "            \n",
        "            output = classifier(combined_features)\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            total += label.size(0)\n",
        "            correct += (predicted == label).sum().item()\n",
        "    \n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Epoch {epoch+1}, Test Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.3 Ensemble stage\n",
        "\n",
        "Describe any ensemble approach you might have included. Explain why you have chosen them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZQPxztuL9AW"
      },
      "source": [
        "# 5 Dataset\n",
        "\n",
        "Describe the datasets that you will create to build and evaluate your models. Your datasets need to be based on our MLEnd Deception Dataset. After describing the datasets, build them here. You can explore and visualise the datasets here as well. \n",
        "\n",
        "If you are building separate training and validatio datasets, do it here. Explain clearly how you are building such datasets, how you are ensuring that they serve their purpose (i.e. they are independent and consist of IID samples) and any limitations you might think of. It is always important to identify any limitations as early as possible. The scope and validity of your conclusions will depend on your ability to understand the limitations of your approach.\n",
        "\n",
        "If you are exploring different datasets, create different subsections for each dataset and give them a name (e.g. 5.1 Dataset A, 5.2 Dataset B, 5.3 Dataset 5.3) .\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qf7GN1aeXJI"
      },
      "source": [
        "# 6 Experiments and results\n",
        "\n",
        "Carry out your experiments here. Analyse and explain your results. Unexplained results are worthless."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSrJCR_cekPO"
      },
      "source": [
        "# 7 Conclusions\n",
        "\n",
        "Your conclusions, suggestions for improvements, etc should go here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 8 References\n",
        "\n",
        "Acknowledge others here (books, papers, repositories, libraries, tools) "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "machine_learning",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
